# %% [markdown]
# # Classifying Data with Logistic Regression in Python
# 
# ## Learning Objectives
# Logistic Regression is one of the simplest and most commonly used classification approaches in machine learning. Logistic regression allows us to model the relationship between independent variables and the probability of a categorical response (such as True or False, Yes or No). By the end of this tutorial, you will have learned:
# 
# + How to import, explore and prepare data
# + How to build a Logistic Regression model
# + How to evaluate a Logistic Regression model
# + How to interpret the coefficients of a Logistic Regression model 

# %% [markdown]
# ## 1. Collect the Data

# %% [markdown]
# Before we import our data, we must first import the `pandas` package.

# %%
import pandas as pd

# %% [markdown]
# Now, we can import our data into a dataframe called `loan`.

# %%
loan = pd.read_csv("loan.csv")

# %% [markdown]
# To verify that the import worked as expected, let’s use the `head()` method of the pandas dataframe to preview the data.

# %%
loan.head()

# %% [markdown]
# Our dataset has three columns. The first two - `Income` and `Loan Amount` - are the predictors (or independent variables), while the last one - `Default` - is the response (or dependent variable).
# 
# In this exercise, we’ll use this `loan` data to train a logistic regression model to predict whether a borrower will default or not default on a new loan based on their income and the amount of money they intend to borrow. 

# %% [markdown]
# ## 2. Explore the Data

# %% [markdown]
# Now that we have our data, let's try to understand it.
# 
# First, let's get a concise summary of the structure of the data by calling the `info()` method of the `loan` dataframe.

# %%
loan.info()

# %% [markdown]
# By looking at the `RangeIndex` value from the summary, we can tell that there are 30 instances (or rows) in the dataset. 
# 
# The `Data columns` value shows that the dataset consists of 3 features (or columns). Looking at the `Dtype` column within this section, we see that the `Income` and `Loan Amount` columns hold integer values, while the `Default` column holds text (aka object).

# %% [markdown]
# Next, let's get summary statistics for the numeric features in the data by calling the `describe()` method of the dataframe.

# %%
loan.describe()

# %% [markdown]
# From the statistics, we can see the average, standard deviation, minimum, and maximum values for both the `Income` and `Loan Amount` variables. We also get the 25th, 50th and 75th percentile values for both variables.
# 
# Note that the values are in the thousands, so the minimum and maximum income values are \\$12,000 and \\$34,000, respectively. 
# 
# Now that we've described our data structurally and numerically, let’s describe it visually as well.

# %% [markdown]
# ### Boxplot
# Before we create the plots we need, we must first import a couple of packages. The first is the `matplotlib` package and the second is the `seaborn` package.

# %%
from matplotlib import pyplot as plt
import seaborn as sns

# %% [markdown]
# Let's start by creating a boxplot that highlights the difference in annual income between those that did not default on their loan (No) and those that did default (Yes). 

# %%
ax = sns.boxplot(data = loan, x = 'Default', y = 'Income')

# %% [markdown]
# The chart shows that those that did not default on their loans tend to have a higher annual income than those that did default on their loans. 

# %% [markdown]
# Next, let's create another box plot to highlight the difference in amount borrowed between those that did not default on their loans and those that did.

# %%
ax = sns.boxplot(data = loan, x = 'Default', y = 'Loan Amount')

# %% [markdown]
# This chart shows that those that defaulted on their loans tend to have borrowed more money than those that did not default.

# %% [markdown]
# ### Scatterplot
# If we recode the `Default` feature values 'No' and 'Yes' to '0' and '1', we can also use a scatterplot to get a slightly different perspective of our data. 

# %% [markdown]
# However, before we do so, we must first import the `numpy` package.

# %%
import numpy as np

# %% [markdown]
# Now, we can create a scatterplot that describes the relationship between the annual income of borrowers and loan outcomes. 

# %%
ax = sns.scatterplot(x = loan['Income'], 
                     y = np.where(loan['Default'] == 'No', 0, 1), 
                     s = 150)

# %% [markdown]
# We can also describe the relationship between the amount borrowed and loan outcomes. 

# %%
ax = sns.scatterplot(x = loan['Loan Amount'], 
                     y = np.where(loan['Default'] == 'No', 0, 1), 
                     s = 150)

# %% [markdown]
# Looking at these two charts, we can easily imagine a sigmoid curve that fits the data. This tells us that a logistic regression function would model the relationship between the predictors (`Income` and `Loan Amount`) and the response (`Default`) well.

# %% [markdown]
# ## 3. Prepare the Data

# %% [markdown]
# Our primary objective in this step is to split our data into training and test sets. The training set will be used to train the model, while the test set will be used to evaluate the model.
# 
# Before we split the data, we first need to separate the dependent variable from the independent variables.

# %% [markdown]
# Let's start by creating a pandas Series called `y` for the dependent variable.

# %%
y = loan['Default']

# %% [markdown]
# Then we create a pandas DataFrame called `X` for the independent variables.

# %%
X = loan[['Income', 'Loan Amount']]

# %% [markdown]
# Next, we import the `train_test_split()` function from the `sklearn.model_selection` subpackage. 

# %%
from sklearn.model_selection import train_test_split

# %% [markdown]
# Using the `train_test_split()` function, we can split `X` and `y` into `X_train`, `X_test`, `y_train` and `y_test`.
# 
# Note that within the `train_test_split()` function, we will set:
# 
# * `train_size` to `0.7`. This means we want $70\%$ of the original data to be assigned to the training data while $30\%$ is assigned to the test data. 
# 
# * `stratify` as `y`, which means that we want the data split using a stratified random sampling approach based on the values of `y`. 
# 
# * `random_state` to `123`, so we get the same results every time we do this split. 

# %%
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    train_size = 0.7,
                                                    stratify = y,
                                                    random_state = 123) 

# %% [markdown]
# After the data is split, the newly created `X_train` and `X_test` data sets hold the independent variables for the training and test sets, respectively. While the `y_train` and `y_test` data sets hold the dependent variable for the training and test sets respectively.
# 

# %% [markdown]
# We can refer to the `shape` attribute of any of the newly created data sets to know how many instances or records are in each. Let's look at the training data.

# %%
X_train.shape

# %% [markdown]
# The result is a tuple that holds the number of rows and columns in the `X_train` dataframe. It tells us that $21$ out of the $30$ instances in the `loans` data were assigned to the training set.
# 
# Let's look at the test set as well.

# %%
X_test.shape

# %% [markdown]
# The result tells us that $9$ out of the $30$ instances in the `loans` data were assigned to the test set.

# %% [markdown]
# ## 4. Train and Evaluate the Model

# %% [markdown]
# We are going to use the `LogisticRegression` class from the `sklearn.linear_model` subpackage to train our model. Let's import it.

# %%
from sklearn.linear_model import LogisticRegression

# %% [markdown]
# We can now instantiate a new object called `classifier` from the `LogisticRegression` class.

# %%
classifier = LogisticRegression()


# %% [markdown]
# To train a model, we pass the training data (`X_train` and `y_train`) to the `fit()` method of the classifier object.

# %%
model = classifier.fit(X_train, y_train)


# %% [markdown]
# Recall that there are $9$ instances (or rows) in the test set. To predict labels for the test instances, we pass the independent variables of the test set (`X_test`) to the `predict()` method of the model.

# %%
model.predict(X_test)

# %% [markdown]
# The output lists the predictions made by the model in the order in which the instances appear in the test set.
# 
# To evaluate how accurate our model is, we pass the test data (`X_test` and `y_test`) to the `score()` method of the model.

# %%
model.score(X_test, y_test)

# %% [markdown]
# The result tells us that our Logistic Regression model is able to correctly predict $8$ out of $9$ (or $89\%$) of the labels in the test set.

# %% [markdown]
# The accuracy of a model only gives us a one-dimensional perspective of performance. To get a broader perspective, we need to generate a confusion (or error) matrix of the model's performance. 
# 
# To do this, we need to import the `confusion_matrix` function from the `sklearn.metrics` subpackage.

# %%
from sklearn.metrics import confusion_matrix

# %% [markdown]
# Then we pass the dependent variable from the test set (which are the actual labels) and the model's predicted labels to the `confusion_matrix()` function.

# %%
confusion_matrix(y_test, model.predict(X_test))

# %% [markdown]
# The result is a $ 2\times 2$ array that shows how many instances the model predicted correctly or incorrectly as either `Yes` or `No`. This confusion matrix can be illustrated as follows:

# %% [markdown]
# ![conf_matrix.png](attachment:conf_matrix.png)

# %% [markdown]
# The first row of the matrix shows that of the $4$ instances that were actually `No`, the model predicted $3$ of them as `No` but $1$ of them as `Yes`. The second row of the matrix shows that of the $5$ instances that were actually `Yes`, the model predicted all $5$ correctly as `Yes`.



